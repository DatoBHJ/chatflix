import { streamText, smoothStream, createDataStreamResponse, streamObject, generateObject, Message } from 'ai';
import { createClient } from '@/utils/supabase/server';
import { providers } from '@/lib/providers';
import { getModelById} from '@/lib/models/config';
import { MultiModalMessage } from './types';
import { z } from 'zod';
import { 
  handlePromptShortcuts,
  saveUserMessage,
  createOrUpdateAssistantMessage,
  handleStreamCompletion,
  buildSystemPrompt
} from './services/chatService';
import { 
  generateMessageId, 
  convertMessageForAI, 
  validateAndUpdateSession,
  getProviderFromModel,
  convertMultiModalToMessage,
  selectMessagesWithinTokenLimit,
  detectImages,
  detectPDFs,
  detectCodeAttachments
} from './utils/messageUtils';
import { 
  createWebSearchTool, 
  createJinaLinkReaderTool, 
  createImageGeneratorTool, 
  createCalculatorTool, 
  createAcademicSearchTool, 
  // createXSearchTool, 
  createYouTubeSearchTool, 
  createYouTubeLinkAnalyzerTool, 
} from './tools';
import { handleRateLimiting, handleChatflixRateLimiting } from './utils/ratelimit';
// import { toolPrompts } from './prompts/toolPrompts';
import { checkSubscription } from '@/lib/polar';

// ë¹„êµ¬ë…ì ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ì œí•œ
const CONTEXT_WINDOW_LIMIT_NON_SUBSCRIBER = 60000; // 60K tokens

// ë©”ëª¨ë¦¬ ê´€ë ¨ import
import { initializeMemoryBank, getAllMemoryBank, getUserPersonalInfo } from '@/utils/memory-bank';
import { smartUpdateMemoryBanks } from './services/memoryService';
import { estimateTokenCount } from '@/utils/context-manager';
import { selectOptimalModel, estimateMultiModalTokens } from './services/modelSelector';
import { 
  analyzeRequestAndDetermineRoute,
  analyzeContextRelevance 
} from './services/analysisService';
import { markdownJoinerTransform } from './markdown-transform';


// Helper function to increment user daily request count
async function incrementSuccessfulRequestCount(
  supabaseClient: any,
  userId: string,
  requestDate: string,
  currentCount: number,
  isUserSubscribed: boolean
) {
  try {
    await supabaseClient
      .from('user_daily_requests')
      .upsert({
        user_id: userId,
        date: requestDate,
        count: currentCount + 1,
        last_request_at: new Date().toISOString(),
        is_subscribed: isUserSubscribed
      }, {
        onConflict: 'user_id,date' 
      });
  } catch (error) {
    // console.error('Failed to update successful request count:', error);
  }
}

// Tool initialization helper function
function initializeTool(type: string, dataStream: any) {
  switch (type) {
    case 'web_search':
      return createWebSearchTool(dataStream);
    case 'calculator':
      return createCalculatorTool(dataStream);
    case 'link_reader':
      return createJinaLinkReaderTool(dataStream);
    case 'image_generator':
      return createImageGeneratorTool(dataStream);
    case 'academic_search':
      return createAcademicSearchTool(dataStream);
    // case 'x_search':
    //   return createXSearchTool(dataStream);
    case 'youtube_search':
      return createYouTubeSearchTool(dataStream);
    case 'youtube_link_analyzer':
      return createYouTubeLinkAnalyzerTool(dataStream);
    default:
      throw new Error(`Unknown tool type: ${type}`);
  }
}

// Helper function to collect tool results from various tools
function collectToolResults(tools: Record<string, any>, toolNames: string[]): any {
  const collectedToolResults: any = {};
  
  toolNames.forEach((toolName: string) => {
    switch (toolName) {
      case 'calculator':
        if (tools.calculator?.calculationSteps?.length > 0) {
          collectedToolResults.calculationSteps = tools.calculator.calculationSteps;
        }
        break;
      case 'web_search':
        if (tools.web_search?.searchResults?.length > 0) {
          collectedToolResults.webSearchResults = tools.web_search.searchResults;
        }
        break;
      case 'link_reader':
        if (tools.link_reader?.linkAttempts?.length > 0) {
          collectedToolResults.linkReaderAttempts = tools.link_reader.linkAttempts;
        }
        break;
      case 'image_generator':
        if (tools.image_generator?.generatedImages?.length > 0) {
          collectedToolResults.generatedImages = tools.image_generator.generatedImages;
        }
        break;
      case 'academic_search':
        if (tools.academic_search?.searchResults?.length > 0) {
          collectedToolResults.academicSearchResults = tools.academic_search.searchResults;
        }
        break;
      case 'youtube_search':
        if (tools.youtube_search?.searchResults?.length > 0) {
          collectedToolResults.youtubeSearchResults = tools.youtube_search.searchResults;
        }
        break;
      case 'youtube_link_analyzer':
        if (tools.youtube_link_analyzer?.analysisResults?.length > 0) {
          collectedToolResults.youtubeLinkAnalysisResults = tools.youtube_link_analyzer.analysisResults;
        }
        break;
    }
  });
  
  return collectedToolResults;
}

async function generateFollowUpQuestions(
  userQuery: string,
  aiResponse: string,
  responseType: 'text' | 'file' = 'text'
): Promise<string[]> {
  try {
    const contextInfo = responseType === 'file' ? 
      'The AI has generated files/documents for the user.' : 
      'The AI has provided a text response to the user.';
    
    const followUpResult = await generateObject({
      model: providers.languageModel('gemini-2.0-flash'),
      prompt: `You are generating follow-up questions that a USER would naturally ask or input to an AI assistant. These should be direct requests, commands, or questions that users would actually type, NOT questions the AI would ask the user.

**CRITICAL INSTRUCTION: Generate user inputs TO the AI, not AI questions TO the user**

User's original query: "${userQuery}"
AI's response: "${aiResponse}"
Context: ${contextInfo}

**WRONG EXAMPLES (AI asking user - DO NOT generate these):**
âŒ "What details would you like me to emphasize in this image?"
âŒ "Which style would you prefer?"
âŒ "Do you want me to modify anything?"
âŒ "Would you like me to create variations?"

**CORRECT EXAMPLES (User asking/requesting from AI - Generate these types):**
âœ… "Create a similar image with a dog instead"
âœ… "Generate a complete code file for this project"
âœ… "Search for the latest news about this topic"
âœ… "How does this algorithm work?"
âœ… "What are the pros and cons of this approach?"
âœ… "Make this image in a different style"
âœ… "Find research papers about this subject"
âœ… "Create a detailed documentation file"
âœ… "Search YouTube for tutorials on this"

**Generate 3 different types of user inputs:**
1. **Action Request**: User asks AI to create, generate, search, or make something
2. **Information Question**: User asks AI to explain, analyze, or provide information
3. **Follow-up Inquiry**: User asks about alternatives, improvements, or related topics

**IMPORTANT RULES:**
- Write as natural user inputs TO the AI (commands, requests, or questions)
- Can be imperative ("Create...") or interrogative ("How does...?", "What is...?")
- Respond in the same language as the user's original query
- Make them natural and actionable - things users would actually type
- Each input should be distinctly different in purpose`,
      schema: z.object({
        followup_questions: z.array(z.string()).length(3)
      })
    });
    
    return followUpResult.object.followup_questions;
  } catch (e) { 
    return [];
  }
}

// Helper function to generate tool execution plan
async function generateToolExecutionPlan(
  userQuery: string,
  selectedTools: string[],
  contextMessages: any[],
  toolDescriptions: Record<string, string>
): Promise<{
  plan: string;
  refinedUserInput: string;
  essentialContext: string;
}> {
  try {
    // ğŸ†• í† í° ì œí•œ ì ìš© (Gemini 2.0 Flashì˜ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê³ ë ¤)
    const maxContextTokens = 1000000; // Gemini 2.0 Flashì˜ ì•ˆì „í•œ í† í° ì œí•œ
    
    // ì²¨ë¶€íŒŒì¼ ê°ì§€
    const hasAttachments = contextMessages.some(msg => {
      if (Array.isArray(msg.content)) {
        return msg.content.some((part: any) => part.type === 'image' || part.type === 'file');
      }
      return Array.isArray(msg.experimental_attachments) && msg.experimental_attachments.length > 0;
    });
    
    // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì²¨ë¶€íŒŒì¼ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
    const currentDate = new Date().toLocaleDateString('en-US', { 
      year: 'numeric', 
      month: 'long', 
      day: 'numeric',
      weekday: 'long'
    });
    
    const systemPrompt = `You are an AI assistant analyzing a user's request to create a detailed execution plan for using tools.

**Current Date:** ${currentDate}
**User's Request:** ${userQuery}
**Selected Tools:** ${selectedTools.join(', ')}
**Available Tools:** ${Object.entries(toolDescriptions).map(([key, desc]) => `${key}: ${desc}`).join('\n')}

${hasAttachments ? `**CRITICAL: ATTACHMENTS DETECTED**
The conversation contains attached files (PDFs, images, documents). You MUST:
1. **Extract ALL specific details** from attached files that are relevant to the user's request
2. **List every specific entity** (companies, people, places, dates, numbers, etc.) mentioned in attachments
3. **Include ALL relevant context** from attachments in your plan
4. **Be extremely detailed** about what information to search for based on attachment content
5. **Mention specific search terms** and entities that should be researched
6. **Consider temporal relevance** - if attachments contain dates, check if information is current as of ${currentDate}

**Example for company-related requests:**
- If user asks "check if mentioned companies are listed" and PDF contains companies A, B, C
- Your plan MUST list: "Search for: Company A, Company B, Company C"
- Include any additional details from PDF about these companies

**Example for data analysis requests:**
- If user asks "analyze this data" and PDF contains specific data points
- Your plan MUST include: "Search for: [specific data points], [relevant metrics], [related trends]"

**ATTACHMENT ANALYSIS REQUIREMENTS:**
- Extract ALL company names, product names, dates, locations, numbers, statistics
- Include ALL relevant keywords and search terms
- Be comprehensive - don't miss any important details
- If attachment contains lists, include ALL items in the list
- If attachment contains tables, extract ALL relevant data points
- **Temporal Analysis**: If attachments contain dates, determine if information needs current verification as of ${currentDate}` : ''}

**Your Task:**
1. **Analyze User Intent**: Understand what the user really wants
2. **Refine User Input**: If the user's input is vague or incomplete, create a more specific version
3. **Create Execution Plan**: Detail how to use each selected tool effectively${hasAttachments ? ', including ALL specific details from attachments' : ''}
4. **Extract Essential Context**: Identify only the most relevant context from conversation history${hasAttachments ? ', with comprehensive details from attached files' : ''}

**Requirements:**
- Respond in the user's language
- Be specific about tool usage order and purpose
- Focus on what will help the user most
- Extract only context that directly relates to the current request
- **Consider temporal relevance** - if the request involves time-sensitive information, prioritize current data as of ${currentDate}${hasAttachments ? `
- **MANDATORY**: Include ALL specific entities, numbers, dates, and details from attachments
- **MANDATORY**: List every search term and entity that should be researched
- **MANDATORY**: Be exhaustive - don't skip any relevant details from attachments
- **MANDATORY**: For any dates in attachments, determine if current verification is needed as of ${currentDate}` : ''}

**Output Format:**
- Plan: Step-by-step tool execution strategy${hasAttachments ? ' with ALL specific details from attachments' : ''}
- Refined User Input: Clear, specific version of user's request
- Essential Context: Only the most relevant parts of conversation history${hasAttachments ? ', including comprehensive attachment details' : ''}`;

    const systemTokens = estimateTokenCount(systemPrompt);
    const remainingTokens = maxContextTokens - systemTokens;
    
    // ğŸ†• í† í° ì œí•œ ë‚´ì—ì„œ ë©”ì‹œì§€ ì„ íƒ
    const optimizedContextMessages = selectMessagesWithinTokenLimit(
      contextMessages,
      remainingTokens,
    );
    
    // ğŸ†• ë©”ì‹œì§€ ë³€í™˜
    const convertedMessages = convertMultiModalToMessage(optimizedContextMessages);
    
    const planResult = await generateObject({
      model: providers.languageModel('gemini-2.0-flash'),
      system: systemPrompt,
      messages: convertedMessages,
      schema: z.object({
        plan: z.string().describe('Detailed step-by-step plan for tool execution'),
        refinedUserInput: z.string().describe('Clear, specific version of user\'s request'),
        essentialContext: z.string().describe('Only the most relevant context from conversation history')
      })
    });
    
    return planResult.object;
  } catch (error) {
    console.error('Failed to generate tool execution plan:', error);
    return {
      plan: 'Execute tools in order to fulfill user request',
      refinedUserInput: userQuery,
      essentialContext: 'No specific context available'
    };
  }
}

export async function POST(req: Request) {
    const supabase = await createClient();
    const { data: { user }, error: userError } = await supabase.auth.getUser();
    
    if (userError || !user) {
      return new Response(JSON.stringify({ error: 'Unauthorized' }), {
        status: 401,
        headers: { 'Content-Type': 'application/json' }
      });
    }

    const requestData = await req.json();
    let { messages, model, chatId, isRegeneration, existingMessageId, saveToDb = true, isAgentEnabled = false } = requestData;

    // Map Chatflix Ultimate model to appropriate model based on agent mode
    if (model === 'chatflix-ultimate' || model === 'chatflix-ultimate-pro') {
        // Store the original model name for DB storage
        requestData.originalModel = model;
        
        try {
          const modelType = model as 'chatflix-ultimate' | 'chatflix-ultimate-pro';
          const { selectedModel } = await selectOptimalModel(messages, modelType);
          model = selectedModel;
        } catch (error) {
          model = 'gemini-2.5-pro';
        }
      }

    // êµ¬ë… ìƒíƒœ í™•ì¸
    const isSubscribed = await checkSubscription(user.id);
    
    // ëª¨ë¸ì´ ë°”ë€ í›„ ìµœì¢… ëª¨ë¸ ì„¤ì •ìœ¼ë¡œ maxContextTokens ì¬ê³„ì‚°
    const finalModelConfig = getModelById(model);
    const maxContextTokens = isSubscribed 
      ? (finalModelConfig?.contextWindow || 120000)
      : CONTEXT_WINDOW_LIMIT_NON_SUBSCRIBER;
    
    // ì‚¬ìš©ìì˜ ì˜¤ëŠ˜ ìš”ì²­ íšŸìˆ˜ í™•ì¸
    const today = new Date().toISOString().split('T')[0]; // YYYY-MM-DD í˜•ì‹
    const { data: userRequests, error: requestsError } = await supabase
      .from('user_daily_requests')
      .select('count')
      .eq('user_id', user.id)
      .eq('date', today)
      .single();
    
    // í˜„ì¬ ìš”ì²­ íšŸìˆ˜ (ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì‹œì‘)
    const currentRequestCount = userRequests?.count || 0;
  
    // ğŸ†• Handle rate limiting based on model type
    const originalModel = requestData.originalModel;
    const isChatflixModel = originalModel === 'chatflix-ultimate' || originalModel === 'chatflix-ultimate-pro';
    
    // rate limit ì²´í¬
    if (isChatflixModel) {
      // Chatflix ëª¨ë¸ì€ ìì²´ rate limitë§Œ ì²´í¬ (ì„ íƒëœ ê°œë³„ ëª¨ë¸ rate limit ë¬´ì‹œ)
      const chatflixRateLimitResult = await handleChatflixRateLimiting(user.id, originalModel, isSubscribed);
      if (!chatflixRateLimitResult.success) {
        const { error } = chatflixRateLimitResult;
        
        if (error) {
          return new Response(JSON.stringify({
            error: 'Too many requests',
            message: error.message,
            retryAfter: error.retryAfter,
            reset: new Date(error.reset).toISOString(),
            limit: error.limit,
            level: error.level,
            model: originalModel, // Use original Chatflix model name
            isSubscribed: isSubscribed // êµ¬ë… ìƒíƒœ í¬í•¨
          }), {
            status: 429,
            headers: {
              'Content-Type': 'application/json',
              'X-RateLimit-Limit': error.limit.toString(),
              'X-RateLimit-Remaining': '0',
              'X-RateLimit-Reset': new Date(error.reset).toISOString(),
            }
          });
        }
      }
    } else {
      // ì¼ë°˜ ëª¨ë¸ì€ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
      const rateLimitResult = await handleRateLimiting(user.id, model, isSubscribed);
      if (!rateLimitResult.success) {
        const { error } = rateLimitResult;
        
        if (error) {
          return new Response(JSON.stringify({
            error: 'Too many requests',
            message: error.message,
            retryAfter: error.retryAfter,
            reset: new Date(error.reset).toISOString(),
            limit: error.limit,
            level: error.level,
            model: model,
            isSubscribed: isSubscribed // êµ¬ë… ìƒíƒœ í¬í•¨
          }), {
            status: 429,
            headers: {
              'Content-Type': 'application/json',
              'X-RateLimit-Limit': error.limit.toString(),
              'X-RateLimit-Remaining': '0',
              'X-RateLimit-Reset': new Date(error.reset).toISOString(),
            }
          });
        } else {
          // Fallback in case error is undefined
          return new Response(JSON.stringify({
            error: 'Too many requests',
            message: 'Rate limit exceeded',
            isSubscribed: isSubscribed // êµ¬ë… ìƒíƒœ í¬í•¨
          }), {
            status: 429,
            headers: {
              'Content-Type': 'application/json'
            }
          });
        }
      }
    }

    return createDataStreamResponse({
      execute: async (dataStream) => {

          let sessionValidationPromise;
          if (chatId) {
            sessionValidationPromise = validateAndUpdateSession(supabase, chatId, user.id, messages);
          } else {
            sessionValidationPromise = Promise.resolve();
          }

          // ë©”ëª¨ë¦¬ ë±…í¬ ì´ˆê¸°í™” (Agent ëª¨ë“œ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´)
          let memoryInitPromise = initializeMemoryBank(
            supabase, 
            user.id,
            // user ê°ì²´ ì „ì²´ ì „ë‹¬
            user
          ).catch(err => {
            // ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
          });
          
          // Process messages in parallel
          const processMessagesPromises = messages.map(async (msg: Message) => {
            const converted = await convertMessageForAI(msg, model, supabase);
            return {
              id: msg.id,
              ...converted
            } as MultiModalMessage;
          });
          
          // ğŸ”§ HIGH PRIORITY OPTIMIZATION: ë³‘ë ¬ ì²˜ë¦¬ í™•ëŒ€
          // Wait for message processing and memory initialization in parallel
          const [
            processMessages
          ] = await Promise.all([
            Promise.all(processMessagesPromises),
            memoryInitPromise
          ]);
          
          // Process last message shortcut if needed
          const lastMessage = processMessages[processMessages.length - 1];
          const processedLastMessage = await handlePromptShortcuts(supabase, lastMessage, user.id) as MultiModalMessage;
          
          // Update the last message with processed shortcuts
          processMessages[processMessages.length - 1] = processedLastMessage;

          // Get memory data in parallel with other operations
          const memoryDataPromise = getAllMemoryBank(supabase, user.id);
          
          // Prepare DB operations (but don't wait)
          let dbOperationsPromise = Promise.resolve();
          
          if (lastMessage.role === 'user' && !isRegeneration && saveToDb && chatId) {
            dbOperationsPromise = new Promise(async (resolve) => {
              try {
                const { data: existingMessages } = await supabase
                  .from('messages')
                  .select('id')
                  .eq('chat_session_id', chatId)
                  .eq('user_id', user.id)
                  .order('created_at', { ascending: true })
                  .limit(2);
                
                const isInitialDbMessage = existingMessages?.length === 1;
                
                if (!isInitialDbMessage) {
                  await saveUserMessage(supabase, chatId, user.id, lastMessage, model);
                }
                resolve(undefined);
              } catch (error) {
                resolve(undefined);
              }
            });
          }

          const assistantMessageId = isRegeneration && existingMessageId 
            ? existingMessageId 
            : generateMessageId();

          if (chatId) {
            dbOperationsPromise = dbOperationsPromise.then(() => 
              createOrUpdateAssistantMessage(
                supabase,
                chatId,
                user.id,
                model,
                getProviderFromModel(model),
                isRegeneration,
                assistantMessageId
              )
            );
          }

          const abortController = new AbortController();

          const modelConfig = getModelById(model);
          const supportsReasoning = modelConfig?.reasoning?.enabled || false;
          // Get max output tokens for this model (if defined)
          // const maxOutputTokens = modelConfig?.maxOutputTokens;

          const providerOptions: any = {};

          // setting provider options
          if (supportsReasoning) {

            providerOptions.groq = {
              reasoningEffort: 'high',
              parallelToolCalls: true,
            };
            
            providerOptions.anthropic = { 
              thinking: { 
                type: 'enabled', 
                budgetTokens: 12000 
              } 
            };
            
            providerOptions.xai = { 
              reasoningEffort: 'high' 
            };
            
            providerOptions.openai = { 
              reasoningEffort: 'high',
            };
            
            providerOptions.google = { 
              thinkingConfig: { 
                thinkingBudget: 2048,
                includeThoughts: true // this shit doesnt work. don't know why.
              }, 
              safetySettings: [
                { category: 'HARM_CATEGORY_HATE_SPEECH', threshold: 'BLOCK_NONE' },
                { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' },
                { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' },
                { category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold: 'BLOCK_NONE' },
              ],
            };
          }
          
          // Get memory data result
          const { data: memoryData } = await memoryDataPromise;
          
          // ë©”ëª¨ë¦¬ ë±…í¬ ë‚´ìš©ì´ ì´ˆê¸°í™” ê°’ì¸ì§€ í™•ì¸
          const isDefaultMemory = memoryData && 
            memoryData.includes('This section contains basic information about the user') &&
            memoryData.includes('This section tracks user preferences such as UI style');
                    
          // 3. í–¥ìƒëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ì í”„ë¡œí•„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€)
          const currentSystemPrompt = buildSystemPrompt(
            isAgentEnabled ? 'agent' : 'regular',
            'TEXT_RESPONSE',
            // ì´ˆê¸° í…œí”Œë¦¿ì¸ ê²½ìš°ì—ëŠ” ì‚¬ìš©ì í”„ë¡œí•„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            isDefaultMemory ? undefined : (memoryData || undefined)
          );
          
          // ğŸ”§ MEDIUM PRIORITY OPTIMIZATION: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ í† í° ê³„ì‚° í•œ ë²ˆë§Œ ìˆ˜í–‰
          const systemTokensCounts = estimateTokenCount(currentSystemPrompt);
          // ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ëœ maxContextTokens ì‚¬ìš©
          let remainingTokens = maxContextTokens - systemTokensCounts;
          
          // ğŸ”§ MEDIUM PRIORITY OPTIMIZATION: ë©”ì‹œì§€ë³„ í† í° ë¯¸ë¦¬ ê³„ì‚° ë° ìºì‹±
          const messagesWithTokens = processMessages.map(msg => {
            const tokenCount = estimateMultiModalTokens(msg as any);
            return {
              ...msg,
              _tokenCount: tokenCount
            };
          });
          
          if (isAgentEnabled) {
            let contextFilter: any | null = null;
            
            // Re-calculate system tokens specifically for agent mode for accuracy
            const agentSystemPromptForCalc = buildSystemPrompt(
              'agent',
              'FILE_RESPONSE', // Use the potentially longest prompt for a safe calculation
              isDefaultMemory ? undefined : (memoryData || undefined)
            );
            const agentSystemTokens = estimateTokenCount(agentSystemPromptForCalc);
            remainingTokens = maxContextTokens - agentSystemTokens;

            const optimizedMessagesForRouting = selectMessagesWithinTokenLimit(
              messagesWithTokens, 
              remainingTokens,
            );

            // ğŸ”§ HIGH PRIORITY OPTIMIZATION: ë©”ì‹œì§€ ë³€í™˜ ì¤‘ë³µ ì œê±°

            // í˜„ì¬ ì§ˆë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì¤€ë¹„
            let userQuery = '';
            
            // ê° ë©”ì‹œì§€ì—ì„œ í…ìŠ¤íŠ¸ ë° ì²¨ë¶€íŒŒì¼ ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜
            const extractTextFromMessage = (msg: any) => {
              if (typeof msg.content === 'string') {
                return msg.content;
              } else if (Array.isArray(msg.content)) {
                // í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ
                const textContent = msg.content
                  .filter((part: any) => part.type === 'text')
                  .map((part: any) => part.text)
                  .join('\n');
                
                // ì²¨ë¶€íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                const attachmentInfo = [];
                
                // ì´ë¯¸ì§€ ì²˜ë¦¬
                const images = msg.content.filter((part: any) => part.type === 'image');
                if (images.length > 0) {
                  attachmentInfo.push(`[ATTACHED: ${images.length} image(s)]`);
                }
                
                // íŒŒì¼ ì²˜ë¦¬
                const files = msg.content.filter((part: any) => part.type === 'file');
                files.forEach((file: any) => {
                  if (file.file) {
                    const fileName = file.file.name || '';
                    const fileType = file.file.contentType || '';
                    
                    // íŒŒì¼ ìœ í˜•ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
                    if (fileType.startsWith('image/') || fileName.match(/\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i)) {
                      attachmentInfo.push(`[ATTACHED: Image file - ${fileName}]`);
                    } else if (fileType === 'application/pdf' || fileName.toLowerCase().endsWith('.pdf')) {
                      attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
                    } else if (fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
                      const extension = fileName.split('.').pop();
                      attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
                    } else {
                      attachmentInfo.push(`[ATTACHED: File - ${fileName} (${fileType})]`);
                    }
                  }
                });
                
                // experimental_attachments ì²˜ë¦¬
                if (Array.isArray(msg.experimental_attachments)) {
                  msg.experimental_attachments.forEach((attachment: any) => {
                    const fileName = attachment.name || '';
                    const fileType = attachment.contentType || attachment.fileType || '';
                    
                    if (fileType === 'image' || fileType.startsWith('image/') || 
                        fileName.match(/\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i)) {
                      attachmentInfo.push(`[ATTACHED: Image file - ${fileName}]`);
                    } else if (fileType === 'pdf' || fileType === 'application/pdf' || 
                              fileName.toLowerCase().endsWith('.pdf')) {
                      attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
                    } else if (fileType === 'code' || 
                              fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
                      const extension = fileName.split('.').pop();
                      attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
                    } else if (fileName) {
                      attachmentInfo.push(`[ATTACHED: File - ${fileName} (${fileType})]`);
                    }
                  });
                }
                
                // í…ìŠ¤íŠ¸ì™€ ì²¨ë¶€íŒŒì¼ ì •ë³´ ê²°í•©
                if (textContent) {
                  return attachmentInfo.length > 0 
                    ? `${textContent}\n${attachmentInfo.join('\n')}` 
                    : textContent;
                } else if (attachmentInfo.length > 0) {
                  return attachmentInfo.join('\n');
                }
              }
              return '';
            };
            
            // í˜„ì¬ ì§ˆë¬¸ë§Œ userQueryì— í• ë‹¹
            const currentMessage = optimizedMessagesForRouting[optimizedMessagesForRouting.length - 1];
            userQuery = extractTextFromMessage(currentMessage);

            // ğŸ†• STEP 0: Parallel Analysis - Context Relevance & Request Routing
            const hasToolResultsInHistory = messagesWithTokens.slice(0, -1).some(msg => 
              (msg as any).tool_results && 
              Object.keys((msg as any).tool_results).some(key => key !== 'token_usage')
            );

            const hasPreviousConversation = messagesWithTokens.length > 1;
            const shouldAnalyzeContext = hasPreviousConversation && hasToolResultsInHistory


            // Define available tools list early for analysis
            let baseAvailableToolsList = [
              'web_search',
              'calculator',
              'link_reader',
              'image_generator',
              'academic_search',
              'youtube_search',
              'youtube_link_analyzer'
            ];

            const analysisModel = 'gemini-2.0-flash';

            // ë„êµ¬ ì„¤ëª… ê°ì²´ ì •ì˜ (ë¶„ì„ì—ì„œ ì‚¬ìš©)
            const toolDescriptions = {
              'web_search': 'Real-time information from the internet',
              'calculator': 'Mathematical calculations and computations',
              'link_reader': 'Reading and analyzing web page content',
              'image_generator': 'Creating visual content',
              'academic_search': 'Finding scholarly and research materials',
              'youtube_search': 'Finding relevant video content',
              'youtube_link_analyzer': 'Analyzing specific YouTube videos'
            };

            // ğŸš€ V6 Plan: New unified analysis and routing
            const [
              routeAnalysisResult,
              contextAnalysisResult
            ] = await Promise.all([
              analyzeRequestAndDetermineRoute(
                analysisModel,
                model,
                baseAvailableToolsList,
                convertMultiModalToMessage(messagesWithTokens, undefined), // Use all messages for routing analysis
                toolDescriptions
              ),
              shouldAnalyzeContext
                ? analyzeContextRelevance(analysisModel, convertMultiModalToMessage(messagesWithTokens, undefined))
                : Promise.resolve(null),
            ]);
            
            // Process context analysis results
            if (contextAnalysisResult) {
              try {
                contextFilter = contextAnalysisResult.object;
              } catch (error) {
                contextFilter = null;
              }
            }
            
            // V7 Optimization: Convert messages ONCE with the final context filter.
            const finalMessagesForAI = convertMultiModalToMessage(messagesWithTokens, contextFilter);
            
            // Recalculate token budget with the now-finalized message list.
            const messagesWithTokensFinal = finalMessagesForAI.map(msg => ({
              ...msg,
              _tokenCount: estimateMultiModalTokens(msg as any)
            }));
            
            const routingDecision = routeAnalysisResult.object;

            const hasImage = messagesWithTokens.some(msg => detectImages(msg));
            const hasFile = messagesWithTokens.some(msg => detectPDFs(msg) || detectCodeAttachments(msg));
            
            switch (routingDecision.route) {
              case 'CLARIFY':
                // Route to ask the user a clarifying question.
                const clarificationResult = streamText({
                  model: providers.languageModel('gemini-2.0-flash'),
                  experimental_transform: [
                    smoothStream({delayInMs: 25}),
                    markdownJoinerTransform(),
                  ],
                  system: `You are Chatflix, a friendly and helpful AI assistant. The user's request needs more information. Your task is to ask the user the clarifying question provided below in a natural, conversational way.

**Core Instruction: ALWAYS respond in the user's language.**

**Instructions:**
- Start with a brief, friendly acknowledgment.
- Then, ask the clarifying question naturally.
- Be conversational and helpful.

**Style Examples (adapt to user's language):**
The following are English examples of the TONE. Do NOT use them literally if the user is not speaking English.
- "I can help with that! First, could you tell me [question]?"
- "Happy to help! I just need a bit more info - [question]"
- "Sure thing! Quick question for you - [question]"

**Bad Examples (wrong tone):**
- Asking the question without any lead-in.
- Being too formal or robotic.

Now, ask the following question in a conversational manner in the user's language: "${routingDecision.question}"`,
                  prompt: `Ask this question naturally: ${routingDecision.question}`,
                  onFinish: async (completion) => {
                    if (abortController.signal.aborted) return;
                    await handleStreamCompletion(
                      supabase,
                      assistantMessageId,
                      user!.id,
                      'gemini-2.0-flash',
                      getProviderFromModel('gemini-2.0-flash'),
                      completion,
                      isRegeneration,
                      { original_model: requestData.originalModel || model, token_usage: completion.usage }
                    );
                  }
                });
                clarificationResult.mergeIntoDataStream(dataStream);
                break;

              case 'TEXT_RESPONSE': {
                // Route A: Generate a complete text-based response, using tools conversationally.
                const tools: Record<string, any> = {};
                routingDecision.tools.forEach((toolName: string) => {
                  tools[toolName] = initializeTool(toolName, dataStream);
                });



                // ğŸ†• STEP 1: Tool Execution Planning (only when tools are selected)
                let executionPlan: any = null;
                let refinedUserInput = userQuery;
                let essentialContext = '';
                
                const needsTools = routingDecision.tools.length > 0;
                
                if (needsTools) {

                  console.log('--------------------------------');
                  console.log('tools', tools);
                  console.log('--------------------------------');
                  
                  // ê³„íš ìˆ˜ë¦½ (ì „ì²´ ë©”ì‹œì§€ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
                  const planResult = await generateToolExecutionPlan(
                    userQuery,
                    routingDecision.tools,
                    messagesWithTokensFinal,
                    toolDescriptions
                  );
                  
                  executionPlan = planResult.plan;
                  refinedUserInput = planResult.refinedUserInput;
                  essentialContext = planResult.essentialContext;

                  console.log('--------------------------------');
                  console.log('executionPlan', executionPlan);
                  console.log('refinedUserInput', refinedUserInput);
                  console.log('essentialContext', essentialContext);
                  console.log('--------------------------------');

                }

                // TEXT_RESPONSE: ë„êµ¬ ì‹¤í–‰ ëª¨ë¸ ê²°ì •
                let toolExecutionModel = model;
                
                // ë„êµ¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ Gemini ëª¨ë¸ì„ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
                if (needsTools) {
                  if (model === 'gemini-2.5-pro') {
                    toolExecutionModel = 'claude-sonnet-4-20250514';
                  } else if (model === 'gemini-2.5-flash') {
                    toolExecutionModel = 'claude-sonnet-4-20250514';
                  } else if (model === 'gemini-2.0-flash') {
                    toolExecutionModel = 'claude-sonnet-4-20250514';
                  }
                }
                
                if (toolExecutionModel !== model) {
                  console.log(`[ëª¨ë¸ ë³€ê²½] ë„êµ¬ ì‹¤í–‰: ${model} â†’ ${toolExecutionModel}`);
                }
                
                // if (toolExecutionModel === 'moonshotai/kimi-k2-instruct') {
                //   console.log(`[ëª¨ë¸ ë³€ê²½] ë„êµ¬ ì‹¤í–‰: moonshotai/kimi-k2-instruct â†’ moonshotai/Kimi-K2-Instruct`);
                //   toolExecutionModel = 'moonshotai/Kimi-K2-Instruct';
                // }

                // ëª¨ë¸ì´ ë°”ë€ ê²½ìš° maxContextTokens ì¬ê³„ì‚°
                const toolExecutionModelConfig = getModelById(toolExecutionModel);
                const toolExecutionMaxContextTokens = isSubscribed 
                  ? (toolExecutionModelConfig?.contextWindow || 120000)
                  : CONTEXT_WINDOW_LIMIT_NON_SUBSCRIBER;

                // ğŸ†• STEP 2: Prepare optimized messages for final execution
                let finalMessagesForExecution: any[];
                let systemPrompt: string;
                
                if (needsTools && executionPlan) {
                  // ë„êµ¬ê°€ ì„ íƒëœ ê²½ìš°: ê³„íš + ì‚¬ìš©ì ìµœì¢… ë©”ì‹œì§€ë§Œ ì „ë‹¬
                  const personalInfo = await getUserPersonalInfo(supabase, user.id);
                  systemPrompt = buildSystemPrompt(
                    'agent', 
                    'TEXT_RESPONSE', 
                    personalInfo || undefined,
                    {
                      selectedTools: routingDecision.tools,
                      executionPlan: executionPlan,
                      refinedUserInput: refinedUserInput,
                      essentialContext: essentialContext
                    }
                  );

                  console.log('--------------------------------');
                  console.log('systemPrompt', systemPrompt);
                  console.log('--------------------------------');

                  const preciseSystemTokens = estimateTokenCount(systemPrompt);
                  const preciseRemainingTokens = toolExecutionMaxContextTokens - preciseSystemTokens;
                  
                  // ì‚¬ìš©ìì˜ ìµœì¢… ë©”ì‹œì§€ë§Œ ì„ íƒ (ê³„íšê³¼ í•¨ê»˜ ì „ë‹¬)
                  const lastMessage = messagesWithTokensFinal[messagesWithTokensFinal.length - 1];
                  const lastMessageTokens = estimateMultiModalTokens(lastMessage as any);
                  
                  if (lastMessageTokens <= preciseRemainingTokens) {
                    finalMessagesForExecution = [convertMultiModalToMessage([lastMessage])[0]];
                  } else {
                    // í† í° ì œí•œì´ ìˆëŠ” ê²½ìš° ë©”ì‹œì§€ ì¶•ì•½
                    const optimizedMessages = selectMessagesWithinTokenLimit(
                      [lastMessage],
                      preciseRemainingTokens,
                    );
                    finalMessagesForExecution = convertMultiModalToMessage(optimizedMessages);
                  }
                  

                } else {
                  // ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš°: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                  // const personalInfo = await getUserPersonalInfo(supabase, user.id);
                  systemPrompt = buildSystemPrompt(
                    'agent', 
                    'TEXT_RESPONSE', 
                    memoryData || undefined,
                    {
                      selectedTools: routingDecision.tools
                    }
                  );

                  const preciseSystemTokens = estimateTokenCount(systemPrompt);
                  const preciseRemainingTokens = toolExecutionMaxContextTokens - preciseSystemTokens;
                  const prefinalMessages = selectMessagesWithinTokenLimit(
                    messagesWithTokensFinal,
                    preciseRemainingTokens,
                  );

                  finalMessagesForExecution = convertMultiModalToMessage(prefinalMessages);
                }

                // console.log('--------------------------------');
                // console.log('finalMessagesForExecution', finalMessagesForExecution);
                // console.log('--------------------------------');

                const textResponsePromise = streamText({
                  model: providers.languageModel(toolExecutionModel),
                  experimental_transform: [
                    smoothStream({delayInMs: 25}),
                    markdownJoinerTransform(),
                  ],
                  system: systemPrompt,
                  messages: finalMessagesForExecution,
                  tools,
                  maxSteps: 20,
                  maxRetries:3,
                  providerOptions,
                  onFinish: async (completion) => {
                    if (abortController.signal.aborted) return;
                    
                    // ğŸ”§ FIX: ë„êµ¬ë³„ ê²°ê³¼ ìˆ˜ì§‘ (í†µí•© í•¨ìˆ˜ ì‚¬ìš©)
                    const collectedToolResults = collectToolResults(tools, routingDecision.tools);
                    
                    // 2. Increment request count
                    await incrementSuccessfulRequestCount(supabase, user!.id, today, currentRequestCount, isSubscribed);

                    // 3. Generate and stream follow-up questions (ê°œì„ ëœ ì „ëµ ì ìš©)
                    const followUpQuestions = await generateFollowUpQuestions(userQuery, completion.text, 'text');
                    
                    const structuredResponse = {
                      response: { 
                        followup_questions: followUpQuestions 
                      }
                    };
                    collectedToolResults.structuredResponse = structuredResponse;
                    
                    // Send as structured_response to match client expectations
                    dataStream.writeMessageAnnotation({
                      type: 'structured_response',
                      data: structuredResponse
                    });
                    
                    // 1. Save main completion to DB (ì´ì œ followup question í¬í•¨)
                    await handleStreamCompletion(
                      supabase,
                      assistantMessageId,
                      user!.id,
                      model,
                      getProviderFromModel(model),
                      completion,
                      isRegeneration,
                      {
                        original_model: requestData.originalModel || model,
                        token_usage: completion.usage,
                        tool_results: collectedToolResults
                      }
                    );

                    // 4. ğŸ†• Smart Memory Update - AI ë¶„ì„ ê¸°ë°˜ ì§€ëŠ¥ì  ì—…ë°ì´íŠ¸
                    setTimeout(async () => {
                      try {
                        await smartUpdateMemoryBanks(
                          supabase, 
                          user!.id, 
                          chatId, 
                          finalMessagesForExecution, 
                          userQuery, 
                          completion.text
                        );
                      } catch (error) {
                        console.error('Smart memory update failed:', error);
                      }
                    }, 1000);
                  }
                });

                textResponsePromise.mergeIntoDataStream(dataStream, { sendReasoning: true });
                      break;
                  }
                  
              case 'FILE_RESPONSE': {
                // Route B: A two-step process to reliably generate files.
                const tools: Record<string, any> = {};
                routingDecision.tools.forEach((toolName: string) => {
                  tools[toolName] = initializeTool(toolName, dataStream);
                });

                // Check if tools are needed
                const needsTools = routingDecision.tools.length > 0;

                // ğŸ†• STEP 1: ë„êµ¬ê°€ ì„ íƒëœ ê²½ìš° ê³„íš ìƒì„± ë‹¨ê³„ ì¶”ê°€
                let executionPlan: string | undefined;
                let refinedUserInput: string | undefined;
                let essentialContext: string | undefined;

                if (needsTools) {
                  try {
                    const planResult = await generateToolExecutionPlan(
                      userQuery,
                      routingDecision.tools,
                      messagesWithTokensFinal,
                      toolDescriptions
                    );

                    executionPlan = planResult.plan;
                    refinedUserInput = planResult.refinedUserInput;
                    essentialContext = planResult.essentialContext;
                  } catch (error) {
                    console.error('Tool execution plan generation failed:', error);
                  }
                }

                // Check if using DeepSeek or Claude Sonnet models (these may take longer for file generation)
                const isSlowerModel = model.toLowerCase().includes('deepseek') || 
                                     (model.includes('claude') && model.includes('sonnet'));
                
                const personalInfo = await getUserPersonalInfo(supabase, user.id);
                const systemPromptForFileStep1 = buildSystemPrompt(
                  'agent',
                  'FILE_STEP1',
                  personalInfo || undefined,
                  {
                    needsTools,
                    isSlowerModel,
                    model,
                    selectedTools: 'tools' in routingDecision ? routingDecision.tools : [],
                    executionPlan,
                    refinedUserInput,
                    essentialContext
                  }
                );

                // FILE_RESPONSE (ë„êµ¬ ì‹¤í–‰ ë‹¨ê³„) - ëª¨ë¸ ê²°ì •ì„ ë¨¼ì € ìˆ˜í–‰
                let toolExecutionModel = model;
                
                // ë„êµ¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ Gemini ëª¨ë¸ì„ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½
                if (needsTools) {
                  if (model === 'gemini-2.5-pro') {
                    toolExecutionModel = 'claude-sonnet-4-20250514';
                  } else if (model === 'gemini-2.5-flash') {
                    toolExecutionModel = 'claude-sonnet-4-20250514';
                  } else if (model === 'gemini-2.0-flash') {
                    toolExecutionModel = 'claude-sonnet-4-20250514';
                  }
                }
                
                if (toolExecutionModel !== model) {
                    console.log(`[ëª¨ë¸ ë³€ê²½] íŒŒì¼ ë„êµ¬ ì‹¤í–‰: ${model} â†’ ${toolExecutionModel}`);
                  }

                // if (toolExecutionModel === 'moonshotai/kimi-k2-instruct') {
                //   console.log(`[ëª¨ë¸ ë³€ê²½] íŒŒì¼ ë„êµ¬ ì‹¤í–‰: moonshotai/kimi-k2-instruct â†’ moonshotai/Kimi-K2-Instruct`);
                //   toolExecutionModel = 'moonshotai/Kimi-K2-Instruct';
                // }

                // ğŸ†• STEP 2: ìµœì¢… ì‹¤í–‰ì„ ìœ„í•œ ë©”ì‹œì§€ ìµœì í™”
                let finalMessagesForExecution: any[];
                let systemPromptForExecution: string;

                if (needsTools && executionPlan && refinedUserInput && essentialContext) {
                  // ê³„íšì´ ìˆëŠ” ê²½ìš°: ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ ì‚¬ìš©
                  finalMessagesForExecution = [lastMessage];
                  systemPromptForExecution = buildSystemPrompt(
                    'agent',
                    'FILE_STEP1',
                    personalInfo || undefined,
                    {
                      needsTools,
                      isSlowerModel,
                      model,
                      selectedTools: routingDecision.tools,
                      executionPlan,
                      refinedUserInput,
                      essentialContext
                    }
                  );
                } else {
                  // ì „í†µ ë°©ì‹: í† í° ì œí•œ ë‚´ì—ì„œ ë©”ì‹œì§€ ì„ íƒ
                  const fileToolExecutionModelConfig = getModelById(toolExecutionModel);
                  const fileToolExecutionMaxContextTokens = isSubscribed 
                    ? (fileToolExecutionModelConfig?.contextWindow || 120000)
                    : CONTEXT_WINDOW_LIMIT_NON_SUBSCRIBER;

                  const preciseSystemTokensFile = estimateTokenCount(systemPromptForFileStep1);
                  const preciseRemainingTokensFile = fileToolExecutionMaxContextTokens - preciseSystemTokensFile;
                  finalMessagesForExecution = selectMessagesWithinTokenLimit(
                    messagesWithTokensFinal,
                    preciseRemainingTokensFile,
                  );
                  systemPromptForExecution = systemPromptForFileStep1;
                }

                const finalMessagesConverted = convertMultiModalToMessage(finalMessagesForExecution);


                if (needsTools) {
                  // Step 1: Execute tools and interact with the user (only if tools are needed)

                  const toolExecutionPromise = streamText({
                    model: providers.languageModel(toolExecutionModel),
                      experimental_transform: [
                        smoothStream({delayInMs: 25}),
                        markdownJoinerTransform(),
                      ],
                      system: systemPromptForExecution,
                      messages: finalMessagesConverted,
                      tools,
                      maxSteps: 20, 
                      maxRetries:3,
                      providerOptions,
                      onFinish: async (toolExecutionCompletion) => {
                        if (abortController.signal.aborted) return;
                        
                        // ğŸ”§ FIX: ë„êµ¬ë³„ ê²°ê³¼ ìˆ˜ì§‘ (FILE_RESPONSE - ë„êµ¬ ì‚¬ìš© ì¼€ì´ìŠ¤, í†µí•© í•¨ìˆ˜ ì‚¬ìš©)
                        const collectedToolResults = collectToolResults(tools, routingDecision.tools);
                        
                        await generateFileWithToolResults(collectedToolResults, toolExecutionCompletion, finalMessagesConverted);
                      }
                    });
                    
                    toolExecutionPromise.mergeIntoDataStream(dataStream, { sendReasoning: true });
                } else {
                  // No tools needed - but still provide a brief explanation before file generation
                  const briefExplanationPromise = streamText({
                    model: providers.languageModel('gemini-2.0-flash'),
                    experimental_transform: [
                      smoothStream({delayInMs: 25}),
                      markdownJoinerTransform(),
                    ],
                    // providerOptions,
                    temperature: 0.0,
                    maxTokens: 3000,
                    system: systemPromptForExecution, // Use the optimized system prompt
                    messages: finalMessagesConverted,
                    onFinish: async (briefCompletion) => {
                      if (abortController.signal.aborted) return;
                      // Call file generation after brief explanation is complete
                      // briefCompletionì„ ì „ë‹¬í•˜ì—¬ ìµœì¢… ì €ì¥ ì‹œ í¬í•¨ì‹œí‚´
                      await generateFileWithToolResults(null, briefCompletion, finalMessagesConverted);
                    }
                  });
                  
                  briefExplanationPromise.mergeIntoDataStream(dataStream, { sendReasoning: true });
                }

                // Helper function to generate files (extracted to avoid code duplication)
                async function generateFileWithToolResults(toolResults: any, stepCompletion: any, messagesForGeneration: any[]) {
                  // Setup progress tracking
                  const startTime = Date.now();
                  let progressCount = 0;
                  let progressInterval: NodeJS.Timeout | null = null;
                  let isFileGenerationComplete = false;
                  let accumulatedContent = ''; // ëˆ„ì ëœ ì»¨í…ì¸  ì €ì¥
                  let sentProgressMessages: string[] = []; // ì „ì†¡ëœ ì§„í–‰ ë©”ì‹œì§€ë“¤ ì¶”ì 
                                
                  // FILE_RESPONSE (íŒŒì¼ ìƒì„± ë‹¨ê³„)
                  let fileGenerationModel = model;
                  if (model === 'moonshotai/kimi-k2-instruct') {
                    fileGenerationModel = 'gpt-4.1';
                  }

                  // Helper function to generate intermediate progress messages
                  async function generateProgressMessage(progressCount: number, userQuery: string, estimatedTimeElapsed: number, memoryData?: string) {
                    try {
                      // ì§„í–‰ ë©”ì‹œì§€ ìƒì„± ì¤‘ë‹¨ ì²´í¬
                      if (isFileGenerationComplete) return null;
                      
                      const progressResult = streamText({
                        model: providers.languageModel('gemini-2.0-flash'),
                        experimental_transform: [
                          smoothStream({delayInMs: 25}),
                          markdownJoinerTransform(),
                        ],
                        system: `You are Chatflix, an AI assistant generating a file for the user. This can take some time, so you need to send a brief, natural-sounding waiting message.

**Core Instruction: ALWAYS respond in the user's language.** Your message should sound like a real person sending a quick text.

**User's Request:** ${userQuery}
**Time Elapsed:** About ${estimatedTimeElapsed} seconds.

**Your Task:**
- Send a short, reassuring message (1 sentence).
- Acknowledge that file generation can take time.
- Vary your message each time.

**Message Type Examples (adapt to user's language):**
Rotate between these types of messages. Do NOT use the English text literally if the user speaks another language.
- **Time Expectation:** "Just a heads-up, this file is taking a moment to generate..."
- **Patience Request:** "Thanks for your patience, still working on this file for you."
- **Process Explanation:** "Still getting everything ready for your file..."
- **Reassurance:** "Still here and working on it! Complex files can sometimes take a bit longer."

**Previously Sent Messages:** ${sentProgressMessages.join(', ')}

${memoryData ? `**User Profile Context:**
${memoryData}

**CRITICAL: Respond in the user's preferred language from their profile. If none, use the language of their query.**` : '**IMPORTANT: Always respond in the language of the user\'s query.**'}

Generate a new, different waiting message.`,
                        prompt: `Brief waiting message #${progressCount}`,
                        temperature: 0.8,
                        maxTokens: 50,
                        onFinish: async (completion) => {
                          // ì´ì¤‘ ì²´í¬: ì™„ë£Œ ì²˜ë¦¬ ì¤‘ì—ë„ ì¤‘ë‹¨ ìƒíƒœ í™•ì¸
                          if (isFileGenerationComplete) return;
                          
                          // ì „ì†¡ëœ ë©”ì‹œì§€ ì¶”ì ì— ì¶”ê°€
                          sentProgressMessages.push(completion.text);
                          
                          // ì§„í–‰ ë©”ì‹œì§€ë¥¼ ëˆ„ì  ì»¨í…ì¸ ì— êµ¬ë¶„ìì™€ í•¨ê»˜ ì¶”ê°€
                          const separator = accumulatedContent ? '\n\n---\n\n' : '';
                          accumulatedContent += separator + completion.text;
                          
                          // ê¸°ì¡´ assistant ë©”ì‹œì§€ë¥¼ ì—…ë°ì´íŠ¸ (ë³„ë„ ë©”ì‹œì§€ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ)
                          await supabase
                            .from('messages')
                            .update({
                              content: accumulatedContent,
                              model: 'gemini-2.0-flash',
                              host: getProviderFromModel('gemini-2.0-flash'),
                              created_at: new Date().toISOString()
                            })
                            .eq('id', assistantMessageId)
                            .eq('user_id', user!.id);
                        }
                      });

                      // ìŠ¤íŠ¸ë¦¼ì„ í´ë¼ì´ì–¸íŠ¸ë¡œ ì „ì†¡
                      progressResult.mergeIntoDataStream(dataStream);
                      
                      return progressResult;
                    } catch (error) {
                      return null;
                    }
                  }

                  // Start progress message timer
                  const startProgressUpdates = () => {
                    const sendProgressMessage = async () => {
                      // ì§„í–‰ ë©”ì‹œì§€ ìƒì„± ìì²´ë¥¼ ì¤‘ë‹¨
                      if (isFileGenerationComplete) return;
                      
                      progressCount++;
                      const elapsedTime = Math.floor((Date.now() - startTime) / 1000);
                      
                      // ë‹¤ì‹œ í•œ ë²ˆ ì²´í¬ (ë¹„ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ)
                      if (isFileGenerationComplete) return;
                      
                      const progressResult = await generateProgressMessage(progressCount, userQuery, elapsedTime, memoryData || undefined);
                    };

                    // Send first progress message after 15 seconds
                    setTimeout(sendProgressMessage, 15000);
                    
                    // Then send progress messages every 90-120 seconds (randomized)
                    const scheduleNextProgressMessage = () => {
                      if (isFileGenerationComplete) return;
                      
                      const randomInterval = 90000 + Math.random() * 30000; // 90-120 seconds
                      progressInterval = setTimeout(async () => {
                        await sendProgressMessage();
                        scheduleNextProgressMessage();
                      }, randomInterval);
                    };
                    
                    scheduleNextProgressMessage();
                  };

                  // Start the progress tracking
                  startProgressUpdates();
                  
                  // Handle abort scenarios
                  if (abortController.signal.aborted) {
                    isFileGenerationComplete = true;
                    if (progressInterval) {
                      clearTimeout(progressInterval);
                      progressInterval = null;
                    }
                    return;
                  }
                  
                  // Step 2: Generate the file using the collected results
                  const fileGenerationSystemPrompt = buildSystemPrompt(
                    'agent', 
                    'FILE_RESPONSE', 
                    memoryData || undefined,
                    {
                      toolResults,
                      hasImage,
                      hasFile,
                      selectedTools: 'tools' in routingDecision ? routingDecision.tools : [] // ì„ íƒëœ ë„êµ¬ ì •ë³´ ì „ë‹¬
                    }
                  );

                  const fileGenerationResult = await streamObject({
                    model: providers.languageModel(fileGenerationModel),
                    system: fileGenerationSystemPrompt,
                    messages: messagesForGeneration,
                    schema: z.object({
                      response: z.object({
                        description: z.string().describe('A casual, friendly sentence to present the files to the user in their language. Sound like a friend handing over completed work. Examples: "All set! Here are your files." or "Perfect! Got everything ready for you." or "Here you go - all done!" Keep it relaxed and casual.'),
                        files: z.array(z.object({
                            name: z.string().describe('Name of the file with appropriate extension.'),
                            content: z.string().describe('COMPREHENSIVE content of the file with ALL details, explanations, and information. This should contain the actual answer to the user\'s request. Format appropriately for the file type. **CRITICAL**: For code files, ALWAYS start with proper code block syntax (```language). Never generate bare code without markdown code blocks!'),
                          })
                        ).describe("Array of files containing ALL the detailed content and answers."),
                      })
                    })
                  });

                  // Stream partial file object to the client for a responsive UI
                  (async () => {
                    let firstPartialReceived = false;
                    for await (const partial of fileGenerationResult.partialObjectStream) {
                      if (abortController.signal.aborted) break;
                      
                      // ì²« ë²ˆì§¸ ìŠ¤íŠ¸ë¦¼ì´ ì‹œì‘ë˜ë©´ ì§„í–‰ ë©”ì‹œì§€ ì¤‘ë‹¨
                      if (!firstPartialReceived) {
                        firstPartialReceived = true;
                        isFileGenerationComplete = true;
                        if (progressInterval) {
                          clearTimeout(progressInterval);
                          progressInterval = null;
                        }
                      }
                      
                      dataStream.writeMessageAnnotation({ type: 'structured_response_progress', data: JSON.parse(JSON.stringify(partial)) });
                    }
                  })();
                  
                  const finalFileObjectFromStream = await fileGenerationResult.object;
                  const fileDescription = finalFileObjectFromStream.response.description || "Here are the files you requested.";

                  // Mark file generation as complete and cleanup progress tracking
                  isFileGenerationComplete = true;
                  if (progressInterval) {
                    clearTimeout(progressInterval);
                    progressInterval = null;
                  }

                  // Start with the base object and add follow-up questions to it.
                  const finalFileObject: any = finalFileObjectFromStream;

                  // Send final structured response and follow-up questions (ê°œì„ ëœ ì „ëµ ì ìš©)
                  const followUpQuestions = await generateFollowUpQuestions(userQuery, fileDescription, 'file');
                  finalFileObject.response.followup_questions = followUpQuestions;
                  
                  dataStream.writeMessageAnnotation({
                    type: 'structured_response',
                    data: finalFileObject
                  });

                  // Manually construct a 'completion' object for saving
                  let finalCompletionForDB;
                  
                  if (stepCompletion) {
                    // Case: Tools were used OR brief explanation was provided
                    const [
                      stepUsage,
                      fileUsage,
                      finishReason
                    ] = await Promise.all([
                      stepCompletion.usage,
                      fileGenerationResult.usage,
                      stepCompletion.finishReason,
                    ]);

                    // ì²« ë²ˆì§¸ ë‹¨ê³„ í…ìŠ¤íŠ¸ë¥¼ ëˆ„ì  ì»¨í…ì¸ ì— ì¶”ê°€
                    if (stepCompletion.text) {
                      const separator = accumulatedContent ? '\n\n---\n\n' : '';
                      accumulatedContent += separator + stepCompletion.text;
                    }

                    // ğŸ”§ FIX: ì´ì „ ë‹¨ê³„(ë„êµ¬ ì‹¤í–‰ ë˜ëŠ” ê°„ë‹¨ ì„¤ëª…)ì˜ í…ìŠ¤íŠ¸ë§Œ í¬í•¨
                    // fileDescriptionì€ ë³„ë„ë¡œ ì¶”ê°€í•˜ì§€ ì•ŠìŒ (ì´ë¯¸ ì§„í–‰ ë©”ì‹œì§€ì— í¬í•¨ë¨)
                    const combinedText = accumulatedContent || fileDescription;

                    finalCompletionForDB = {
                      text: combinedText,
                      usage: {
                        promptTokens: (stepUsage.promptTokens || 0) + (fileUsage.promptTokens || 0),
                        completionTokens: (stepUsage.completionTokens || 0) + (fileUsage.completionTokens || 0),
                        totalTokens: (stepUsage.totalTokens || 0) + (fileUsage.totalTokens || 0),
                      },
                      finishReason: finishReason
                    };
                  } else {
                    // Case: No tools were used but brief explanation was provided
                    if (stepCompletion && stepCompletion.text) {
                      const separator = accumulatedContent ? '\n\n---\n\n' : '';
                      accumulatedContent += separator + stepCompletion.text;
                    }
                    
                    const fileUsage = await fileGenerationResult.usage;
                    const finalText = accumulatedContent || fileDescription;
                    
                    finalCompletionForDB = {
                      text: finalText,
                      usage: fileUsage,
                      finishReason: 'stop'
                    };
                  }
                  
                  // ğŸ”§ FIX: ë„êµ¬ ê²°ê³¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬
                  let finalToolResults: any = {
                    structuredResponse: finalFileObject
                  };
                  
                  // toolResultsê°€ ì´ë¯¸ ìˆ˜ì§‘ëœ ë„êµ¬ ê²°ê³¼ë¼ë©´ ê·¸ê²ƒì„ ì‚¬ìš©
                  if (toolResults && typeof toolResults === 'object' && !Array.isArray(toolResults)) {
                    // toolResultsì— ì´ë¯¸ ì‚¬ìš©ì ì •ì˜ ê²°ê³¼ë“¤ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´ ë³‘í•©
                    finalToolResults = {
                      ...toolResults,
                      structuredResponse: finalFileObject
                    };
                  }

                  // ì´ë¯¸ finalCompletionForDB.textì— ëª¨ë“  ëˆ„ì  ì»¨í…ì¸ ê°€ í¬í•¨ë˜ì–´ ìˆìŒ

                  // Finalize the process (save to DB, increment count, update memory)
                  await handleStreamCompletion(
                    supabase,
                    assistantMessageId,
                    user!.id,
                    fileGenerationModel,
                    getProviderFromModel(fileGenerationModel),
                    finalCompletionForDB as any,
                    isRegeneration,
                    { 
                      original_model: requestData.originalModel || model,
                      token_usage: finalCompletionForDB.usage,
                      tool_results: finalToolResults
                    }
                  );

                  await incrementSuccessfulRequestCount(supabase, user!.id, today, currentRequestCount, isSubscribed);
                  
                  // ğŸ†• Smart Memory Update for file generation
                  setTimeout(async () => {
                    try {
                      await smartUpdateMemoryBanks(
                        supabase, 
                        user!.id, 
                        chatId, 
                        finalMessagesForExecution, 
                        userQuery, 
                        fileDescription
                      );
                    } catch (error) {
                      console.error('Smart memory update failed:', error);
                    }
                  }, 1000);
                }
                
                break;
              }
            }
            // =================================================================
            // END: NEW V6 LOGIC
            // =================================================================

          } else {
            // ì¼ë°˜ ì±„íŒ… íë¦„ - ì›ë˜ ì½”ë“œ ì‚¬ìš©ì— í† í° ì œí•œ ìµœì í™” ì¶”ê°€
            //  ì´ë¯¸ ê³„ì‚°ëœ ì‹œìŠ¤í…œ í† í° ì¬ì‚¬ìš©

            const optimizedMessages = selectMessagesWithinTokenLimit(
              messagesWithTokens, 
              remainingTokens,
            );

            const messages = convertMultiModalToMessage(optimizedMessages);

            const result = streamText({
              model: providers.languageModel(model),
              experimental_transform: [
                smoothStream({delayInMs: 25}),
                markdownJoinerTransform(),
              ],
              system: currentSystemPrompt, // Use the 'regular' prompt calculated earlier
              messages: messages,
              // temperature: 0.7,
              // maxTokens: 20000,
              providerOptions: providerOptions,
              onFinish: async (completion) => {
                if (abortController.signal.aborted) return;

                // ğŸ†• ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ì¶œ ë° ë¡œê¹…
                const actualTokenUsage = completion.usage;
                // if (actualTokenUsage) {
                //   console.log('ğŸ”¢ [TOKEN USAGE] Regular mode actual tokens:', {
                //     promptTokens: actualTokenUsage.promptTokens,
                //     completionTokens: actualTokenUsage.completionTokens,
                //     totalTokens: actualTokenUsage.totalTokens,
                //     model: model,
                //     messageId: assistantMessageId
                //   });
                // }

                await handleStreamCompletion(
                  supabase,
                  assistantMessageId,
                  user!.id,
                  model,
                  getProviderFromModel(model),
                  completion,
                  isRegeneration,
                  { 
                    original_model: requestData.originalModel || model,
                    token_usage: actualTokenUsage // ğŸ†• ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ì¶”ê°€
                  }
                );

                // Increment daily request count only on successful, non-aborted completion
                if (!abortController.signal.aborted) {
                  await incrementSuccessfulRequestCount(
                    supabase,
                    user!.id,
                    today,
                    currentRequestCount,
                    isSubscribed
                  );
                }

                // ğŸ†• Smart Memory Update for regular chat
                if (chatId && !abortController.signal.aborted) {
                  // AIì˜ ì‘ë‹µê³¼ ì‚¬ìš©ì ë©”ì‹œì§€ ì¤€ë¹„
                  const userMessage = typeof processedLastMessage.content === 'string' 
                    ? processedLastMessage.content 
                    : JSON.stringify(processedLastMessage.content);
                  const aiMessage = completion.text;
                  
                  // 1ì´ˆ ë”œë ˆì´ë¡œ Smart ì—…ë°ì´íŠ¸ ì‹¤í–‰
                  setTimeout(async () => {
                    try {
                      await smartUpdateMemoryBanks(
                        supabase, 
                        user!.id, 
                        chatId, 
                        optimizedMessages, 
                        userMessage, 
                        aiMessage
                      );
                    } catch (error) {
                      console.error('Smart memory update failed:', error);
                    }
                  }, 1000);
                }
              }
            });

            result.mergeIntoDataStream(dataStream, {
              sendReasoning: true
            });

          }

      }
          });
}


