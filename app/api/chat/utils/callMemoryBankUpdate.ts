/**
 * Utility function for making API calls to AI providers for memory bank updates
 */

/**
 * í´ë°± ë©”ëª¨ë¦¬ ì»¨í…ì¸  ìƒì„± (OpenAI API ì‚¬ìš© ë¶ˆê°€ì‹œ)
 */
function generateFallbackMemoryContent(systemPrompt: string, userPrompt: string): string {
  const currentDate = new Date().toLocaleDateString();
  
  // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ì„œ ë©”ëª¨ë¦¬ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ ì‹œë„
  let category = 'general';
  if (systemPrompt.includes('personal core') || userPrompt.includes('personal core')) {
    category = 'personal-core';
  } else if (systemPrompt.includes('interest core') || userPrompt.includes('primary interests')) {
    category = 'interest-core';
  } else if (systemPrompt.includes('active context') || userPrompt.includes('current focus')) {
    category = 'active-context';
  }
  
  return `# ${category.charAt(0).toUpperCase() + category.slice(1)} (Fallback Mode)

âš ï¸ **Note**: This memory entry was created in fallback mode due to AI service unavailability.

## Last Updated
${currentDate}

## Status
Memory update temporarily unavailable - AI analysis service offline.
Basic structure maintained for future updates.

## Notes
- Full analysis will be performed when AI service is restored
- User activity continues to be tracked
- This placeholder ensures memory structure consistency
`;
}

/**
 * Makes a call to the Google AI API with the given prompts to update user memory
 * 
 * @param model - The model to use (e.g., 'gemini-2.5-flash')
 * @param systemPrompt - The system prompt for context
 * @param userPrompt - The user prompt with the actual query
 * @param maxTokens - Maximum tokens to generate (default: 500)
 * @param responseSchema - Optional JSON schema for structured output (guarantees JSON response)
 * @returns The generated content or null if the request fails
 */
export async function callMemoryBankUpdate(
  model: string, 
  systemPrompt: string, 
  userPrompt: string, 
  maxTokens: number = 500, 
  responseSchema?: object
): Promise<string | null> {
  try {
    // Check if API key exists
    if (!process.env.GOOGLE_GENERATIVE_AI_API_KEY) {
      // ğŸ†• ê°„ë‹¨í•œ í´ë°± ë©”ì»¤ë‹ˆì¦˜ ì œê³µ
      return generateFallbackMemoryContent(systemPrompt, userPrompt);
    }

    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${process.env.GOOGLE_GENERATIVE_AI_API_KEY}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: `${systemPrompt}\n\n${userPrompt}`
          }]
        }],
        generationConfig: {
          maxOutputTokens: maxTokens,
          thinkingConfig: {
            thinking_budget: 0
          },
          ...(responseSchema && {
            responseMimeType: "application/json",
            responseSchema: responseSchema
          })
        }
      })
    });
    
    if (!response.ok) {
      const errorData = await response.text();
      
      // ğŸ†• íŠ¹ì • ì—ëŸ¬ ì½”ë“œì— ëŒ€í•œ í´ë°± ì œê³µ
      if (response.status === 429 || response.status >= 500) {
        return generateFallbackMemoryContent(systemPrompt, userPrompt);
      }
      
      return null;
    }
    
    const data = await response.json();
    const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
    
    return content;
  } catch (error) {
    return null;
  }
} 
