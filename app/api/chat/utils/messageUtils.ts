import { type ModelMessage } from 'ai';
import { getModelById } from '@/lib/models/config';
import { providers } from '@/lib/providers';
import { generateObject, convertToModelMessages } from 'ai';
import { z } from 'zod';

export const generateMessageId = () => `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;

type UIMessageCompat = {
  id: string;
  role: 'system' | 'user' | 'assistant' | string;
  content?: any;
  experimental_attachments?: any[];
  parts?: any[];
  metadata?: any;
  [key: string]: any;
};

export const fetchFileContent = async (url: string, supabase?: any, fileType?: string): Promise<{ text?: string; base64?: string } | null> => {
  try {
    // Special handling for PDFs - always get binary data
    const isPDF = fileType === 'pdf' || url.includes('.pdf') || url.includes('application/pdf');
    
    console.log('ğŸ” [DEBUG] fetchFileContent called:', {
      fileType: fileType,
      isPDF: isPDF,
      urlPreview: url.substring(0, 50) + '...'
    });
    
    if (url.includes('chat_attachments') && supabase) {
      const filePath = url.split('chat_attachments/')[1]?.split('?')[0];
      if (filePath) {
        try {
          const { data, error } = await supabase.storage
            .from('chat_attachments')
            .download(filePath);
            
          if (error) {
            return null;
          }
          
          if (isPDF) {
            // For PDFs, return base64 encoded data
            const arrayBuffer = await data.arrayBuffer();
            const bytes = new Uint8Array(arrayBuffer);
            const binary = bytes.reduce((acc, byte) => acc + String.fromCharCode(byte), '');
            const base64Data = btoa(binary);
            console.log('ğŸ” [DEBUG] Returning PDF as base64, length:', base64Data.length);
            return { base64: base64Data };
          } else {
            // For text files, return text content
            const textContent = await data.text();
            console.log('ğŸ” [DEBUG] Returning text content, length:', textContent.length);
            return { text: textContent };
          }
        } catch (err) {
          return null;
        }
      }
    }
    
    if (url.startsWith('http') || url.startsWith('https')) {
      const response = await fetch(url);
      if (!response.ok) {
        return null;
      }
      
      if (isPDF) {
        // For PDFs, return base64 encoded data
        const arrayBuffer = await response.arrayBuffer();
        const bytes = new Uint8Array(arrayBuffer);
        const binary = bytes.reduce((acc, byte) => acc + String.fromCharCode(byte), '');
        const base64Data = btoa(binary);
        return { base64: base64Data };
      } else {
        // For text files, return text content
        return { text: await response.text() };
      }
    }
    else if (url.startsWith('data:')) {
      // Data URLs are already properly formatted, no need to modify
      if (url.includes('base64,')) {
        // Extract base64 part if it's already in that format
        const base64Content = url.split(',')[1];
        if (base64Content) {
          if (isPDF) {
            return { base64: base64Content };
          } else {
            return { text: atob(base64Content) };
          }
        }
      }
    }
    return null;
  } catch (error) {
    return null;
  }
};


export const validateAndUpdateSession = async (
  supabase: any,
  chatId: string,
  userId: string,
  messages: any[]
) => {
  // ğŸš€ ìµëª… ì‚¬ìš©ì ì§€ì›: ìµëª… ì‚¬ìš©ìëŠ” ì„¸ì…˜ ê²€ì¦ ê±´ë„ˆë›°ê¸°
  if (userId === 'anonymous' || userId.startsWith('anonymous_')) {
    return;
  }

  try {
    // ğŸš€ ì„¸ì…˜ê³¼ ë©”ì‹œì§€ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒ
    const [sessionResult, messagesResult] = await Promise.all([
      supabase
        .from('chat_sessions')
        .select()
        .eq('id', chatId)
        .eq('user_id', userId)
        .single(),
      supabase
        .from('messages')
        .select('*')
        .eq('chat_session_id', chatId)
        .eq('user_id', userId)
        .order('sequence_number', { ascending: true })
    ]);

    const { data: existingSession, error: sessionError } = sessionResult;
    const { data: sessionMessages, error: messagesError } = messagesResult;

    // ğŸš€ ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ì²˜ë¦¬ (ìƒˆ ì±„íŒ…ì¸ ê²½ìš°)
    if (sessionError || !existingSession) {
      return;
    }

    // ğŸš€ ë©”ì‹œì§€ ë™ê¸°í™” ìµœì í™” (Map ì‚¬ìš©)
    if (!messagesError && sessionMessages) {
      const messageMap = new Map(sessionMessages.map((msg: any) => [msg.id, msg]));
      
      messages.forEach((msg, index) => {
        const dbMessage = messageMap.get(msg.id) as any;
        if (dbMessage) {
          // í•„ìš”í•œ ë°ì´í„°ë§Œ ì„ íƒì ìœ¼ë¡œ ë™ê¸°í™”
          if (dbMessage.is_edited) {
            messages[index].content = dbMessage.content;
          }
          if (dbMessage.experimental_attachments?.length > 0) {
            messages[index].experimental_attachments = dbMessage.experimental_attachments;
          }
          
          // ğŸ†• ìƒˆë¡œìš´ token_usage ì¹¼ëŸ¼ì—ì„œ í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ ë¡œë“œ
          if (dbMessage.token_usage) {
            (messages[index] as any).token_usage = dbMessage.token_usage;
          }
          
          // Include tool_results from the database message
          if (dbMessage.tool_results) {
            (messages[index] as any).tool_results = dbMessage.tool_results;
            
            // ğŸ†• ë°±ì›Œë“œ í˜¸í™˜ì„±: tool_resultsì— token_usageê°€ ìˆê³  dedicated columnì— ì—†ëŠ” ê²½ìš° ì²˜ë¦¬
            if (dbMessage.tool_results.token_usage && !dbMessage.token_usage) {
              (messages[index] as any).token_usage = dbMessage.tool_results.token_usage;
            }
          }
        }
      });
    }
  } catch (error) {
    // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì¡°ìš©íˆ ì²˜ë¦¬ (AI ì‘ë‹µì— ì˜í–¥ ì—†ìŒ)
    console.log('ğŸš€ [SESSION] Session validation failed:', (error as Error).message);
  }
};

export const getProviderFromModel = (model: string): string => {
  const selectedModel = providers.languageModel(model);
  return selectedModel?.provider || 'Unknown Provider';
};


// ğŸ†• ê°ì§€ í•¨ìˆ˜ë“¤ (modelSelectorì™€ ë™ì¼í•œ ë¡œì§)
export function detectImages(message: any): boolean {

  
  // AI SDK v5: parts ë°°ì—´ êµ¬ì¡° ì²´í¬
  if (Array.isArray(message.parts)) {
    const hasImage = message.parts.some((part: any) => part.type === 'image');
    console.log('ğŸ” [DEBUG] detectImages from parts:', hasImage);
    return hasImage;
  }
  
  return false;
}

export function detectPDFs(message: any): boolean {
  // AI SDK v5: parts ë°°ì—´ êµ¬ì¡° ì²´í¬
  if (Array.isArray(message.parts)) {
    return message.parts.some((part: any) => 
      part.type === 'file' && 
      (part.mimeType === 'application/pdf' || 
       (part.filename && part.filename.toLowerCase().endsWith('.pdf')))
    );
  }
  return false;
}

export function detectCodeAttachments(message: any): boolean {
  return Array.isArray(message.experimental_attachments) && 
    message.experimental_attachments.some((attachment: any) => 
      attachment.fileType === 'code' || 
      (attachment.name && attachment.name.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|scala|swift|kt|rs|dart|json|xml|yaml|yml)$/i))
    );
}

/**
 * ë©”ì‹œì§€ì—ì„œ í…ìŠ¤íŠ¸ì™€ ì²¨ë¶€íŒŒì¼ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
 * AI SDK v5ì˜ ë‹¤ì–‘í•œ ë©”ì‹œì§€ êµ¬ì¡°ë¥¼ ì§€ì›
 */
export const extractTextFromMessage = (msg: any): string => {
  if (typeof msg.content === 'string') {
    return msg.content;
  } else if (Array.isArray(msg.content)) {
    // í…ìŠ¤íŠ¸ ë¶€ë¶„ ì¶”ì¶œ
    const textContent = msg.content
      .filter((part: any) => part.type === 'text')
      .map((part: any) => part.text)
      .join('\n');
    
    // ì²¨ë¶€íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
    const attachmentInfo = [];
    
    // ì´ë¯¸ì§€ ì²˜ë¦¬
    const images = msg.content.filter((part: any) => part.type === 'image');
    if (images.length > 0) {
      attachmentInfo.push(`[ATTACHED: ${images.length} image(s)]`);
    }
    
    // íŒŒì¼ ì²˜ë¦¬
    const files = msg.content.filter((part: any) => part.type === 'file');
    files.forEach((file: any) => {
      if (file.file) {
        const fileName = file.file.name || '';
        const fileType = file.file.contentType || '';
        
        // íŒŒì¼ ìœ í˜•ì— ë”°ë¥¸ êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
        if (fileType.startsWith('image/') || fileName.match(/\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i)) {
          attachmentInfo.push(`[ATTACHED: Image file - ${fileName}]`);
        } else if (fileType === 'application/pdf' || fileName.toLowerCase().endsWith('.pdf')) {
          attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
        } else if (fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
          const extension = fileName.split('.').pop();
          attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
        } else {
          attachmentInfo.push(`[ATTACHED: File - ${fileName} (${fileType})]`);
        }
      }
    });
    
    // AI SDK 5: parts ë°°ì—´ êµ¬ì¡° ì²˜ë¦¬
    if (Array.isArray(msg.parts)) {
      msg.parts.forEach((part: any) => {
        if (part.type === 'image') {
          attachmentInfo.push(`[ATTACHED: Image file]`);
        } else if (part.type === 'file') {
          const fileName = part.filename || '';
          const mediaType = part.mediaType || '';
          
          if (mediaType === 'application/pdf' || fileName.toLowerCase().endsWith('.pdf')) {
            attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
          } else if (fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
            const extension = fileName.split('.').pop();
            attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
          } else if (fileName) {
            attachmentInfo.push(`[ATTACHED: File - ${fileName} (${mediaType})]`);
          }
        }
      });
    }
    
    // í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ experimental_attachments ì²˜ë¦¬
    if (Array.isArray((msg as any).experimental_attachments)) {
      (msg as any).experimental_attachments.forEach((attachment: any) => {
        const fileName = attachment.name || '';
        const fileType = attachment.contentType || attachment.fileType || '';
        
        if (fileType === 'image' || fileType.startsWith('image/') || 
            fileName.match(/\.(jpg|jpeg|png|gif|bmp|webp|svg)$/i)) {
          attachmentInfo.push(`[ATTACHED: Image file - ${fileName}]`);
        } else if (fileType === 'pdf' || fileType === 'application/pdf' || 
                  fileName.toLowerCase().endsWith('.pdf')) {
          attachmentInfo.push(`[ATTACHED: PDF document - ${fileName}]`);
        } else if (fileType === 'code' || 
                  fileName.match(/\.(js|ts|jsx|tsx|py|java|c|cpp|cs|go|rb|php|html|css|sql|swift|kt|rs|dart|json|xml|yaml|yml)$/i)) {
          const extension = fileName.split('.').pop();
          attachmentInfo.push(`[ATTACHED: Code file (${extension}) - ${fileName}]`);
        } else if (fileName) {
          attachmentInfo.push(`[ATTACHED: File - ${fileName} (${fileType})]`);
        }
      });
    }
    
    // í…ìŠ¤íŠ¸ì™€ ì²¨ë¶€íŒŒì¼ ì •ë³´ ê²°í•©
    if (textContent) {
      return attachmentInfo.length > 0 
        ? `${textContent}\n${attachmentInfo.join('\n')}` 
        : textContent;
    } else if (attachmentInfo.length > 0) {
      return attachmentInfo.join('\n');
    }
  }
  return '';
};



// OG Follow Up Questions
// export async function generateFollowUpQuestions(
//   userQuery: string,
//   aiResponse: string,
//   userMemoryData?: string
// ): Promise<string[]> {
//   try {
//     const contextInfo = 'The AI has provided a text response to the user.';
    
//     // ì‚¬ìš©ì ë©”ëª¨ë¦¬ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
//     const personalizationContext = userMemoryData ? `
// **USER MEMORY & PREFERENCES:**
// ${userMemoryData}

// **PERSONALIZATION INSTRUCTIONS:**
// - Consider the user's interests, preferences, and communication style from their memory data
// - Generate questions that align with their typical topics of interest
// - Use their preferred level of detail and technical depth
// - Match their communication style (formal/casual, technical/non-technical)
// - Consider their past interactions and what they typically ask about
// - If they have specific domains of expertise or interest, incorporate those
// - Use their preferred language style and terminology` : '';

//     const followUpResult = await generateObject({
//       model: providers.languageModel('gemini-2.0-flash'),
//       prompt: `You are generating follow-up questions that a USER would naturally ask or input to an AI assistant. These should be direct requests, commands, or questions that users would actually type, NOT questions the AI would ask the user.

// **CRITICAL INSTRUCTION: Generate user inputs TO the AI, not AI questions TO the user**

// User's original query: "${userQuery}"
// AI's response: "${aiResponse}"
// Context: ${contextInfo}${personalizationContext}

// **WRONG EXAMPLES (AI asking user - DO NOT generate these):**
// âŒ "What details would you like me to emphasize in this image?"
// âŒ "Which style would you prefer?"
// âŒ "Do you want me to modify anything?"
// âŒ "Would you like me to create variations?"

// **CORRECT EXAMPLES (User asking/requesting from AI - Generate these types):**
// âœ… "Create a similar image with a dog instead"
// âœ… "Search for the latest news about this topic"
// âœ… "How does this algorithm work?"
// âœ… "What are the pros and cons of this approach?"
// âœ… "Make this image in a different style"
// âœ… "Find research papers about this subject"
// âœ… "Search YouTube for tutorials on this"
// âœ… "Explain this concept in more detail"
// âœ… "What are the alternatives to this approach?"

// **Generate 3 different types of user inputs:**
// 1. **Action Request**: User asks AI to create, generate, search, or make something
// 2. **Information Question**: User asks AI to explain, analyze, or provide information
// 3. **Follow-up Inquiry**: User asks about alternatives, improvements, or related topics

// **IMPORTANT RULES:**
// - Write as natural user inputs TO the AI (commands, requests, or questions)
// - Can be imperative ("Create...") or interrogative ("How does...?", "What is...?")
// - Respond in the same language as the user's original query
// - Make them natural and actionable - things users would actually type
// - Each input should be distinctly different in purpose
// - If user memory data is available, personalize questions based on their interests and communication style
// - Consider their technical level, preferred topics, and past interaction patterns`,
//       schema: z.object({
//         followup_questions: z.array(z.string()).length(3)
//       })
//     });
    
//     return followUpResult.object.followup_questions;
//   } catch (e) { 
//     console.error('Error generating follow-up questions:', e);
//     return [];
//   }
// }


/**
 * í›„ì† ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜
 */
export async function generateFollowUpQuestions(
  userQuery: string,
  aiResponse: string,
  userMemoryData?: string
): Promise<string[]> {
  try {
    const contextInfo = 'The AI has provided a text response to the user.';
    
    // ì‚¬ìš©ì ë©”ëª¨ë¦¬ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    const personalizationContext = userMemoryData ? `
**USER MEMORY & PREFERENCES:**
${userMemoryData}

**PERSONALIZATION INSTRUCTIONS:**
- Consider the user's interests, preferences, and communication style from their memory data
- Generate questions that align with their typical topics of interest
- Use their preferred level of detail and technical depth
- Match their communication style (formal/casual, technical/non-technical)
- Consider their past interactions and what they typically ask about
- If they have specific domains of expertise or interest, incorporate those
- Use their preferred language style and terminology` : '';

    const followUpResult = await generateObject({
      model: providers.languageModel('gemini-2.0-flash'),
      prompt: `You are generating follow-up questions that a USER would naturally ask to continue the conversation with an AI assistant. These should be questions that naturally flow from the current response and help deepen the discussion.

**CRITICAL INSTRUCTION: Generate natural conversation continuations that build upon the current response**

User's original query: "${userQuery}"
AI's response: "${aiResponse}"
Context: ${contextInfo}${personalizationContext}

**CONVERSATION FLOW PRINCIPLES:**
- Questions should naturally continue the current topic or explore related aspects
- Focus on deepening understanding, exploring implications, or connecting to broader themes
- Questions should feel like a natural next step in the conversation
- Avoid questions that feel like starting a completely new topic
- Questions should build upon the information just provided
- Make questions "click-worthy" by including subtle summaries of key elements and sparking curiosity â€“ keep them short and intriguing

**WRONG EXAMPLES (Don't generate these):**
âŒ "What details would you like me to emphasize in this image?"
âŒ "Which style would you prefer?"
âŒ "Do you want me to modify anything?"
âŒ "Would you like me to create variations?"
âŒ Questions that feel like starting a completely unrelated topic
âŒ "What are the ethical implications of this technology?"  // Avoid boring ethical questions
âŒ "How can we solve the privacy issues here?"  // Too unrelated and not engaging
âŒ "What countries might adopt this first?"  // Too obvious and not curiosity-driven

**CORRECT EXAMPLES (Generate these types):**
âœ… "You mentioned 125,000-word vocab â€“ any cases where it perfectly matched thoughts?"  // Summarizes key detail, builds curiosity
âœ… "Â£10,000 robot pregnancy vs real hospital costs â€“ how much cheaper?"  // Engaging real-life comparison
âœ… "60-year zombie satellite signal â€“ what's it actually saying now?"  // Stimulating with keywords
âœ… "37.6% indoor solar efficiency â€“ how much could it cut my home electric bill?"  // Practical and click-worthy
âœ… "Brad Pitt burglary â€“ what exactly did they steal?"  // Intriguing detail request
âœ… "AI-generated images got 90 years â€“ what did the guy do to get caught?"  // Mysterious angle

**Generate 3 different types of conversation continuations:**
1. **Deepening Questions**: Questions that explore the current topic in more detail, depth, or complexity
2. **Connection Questions**: Questions that connect the current topic to related concepts, applications, or broader themes
3. **Practical Questions**: Questions that explore real-world applications, implications, or next steps

**IMPORTANT RULES:**
- Write as natural conversation continuations that flow from the current response
- Questions should feel like logical next steps in the discussion
- Can be interrogative ("How does...?", "What are...?") or exploratory ("Tell me more about...", "Explain how...")
- Respond in the same language as the user's original query
- Make them feel like natural conversation flow, not isolated requests
- Each question should explore a different aspect or direction of the current topic
- If user memory data is available, personalize questions based on their interests and communication style
- Consider their technical level, preferred topics, and past interaction patterns
- Questions should encourage further exploration and discussion
- Include key phrases or details from the AI response to subtly summarize and build curiosity
- Prioritize engaging, fun, or mysterious angles (e.g., specific examples, real-life calculations, surprising facts) to make them "click-worthy"
- Avoid boring, overly ethical, or obvious questions; focus on rabbit hole-style depth that users would naturally pursue
- Keep questions concise (under 20-30 words) and punchy to encourage quick clicks`,
      schema: z.object({
        followup_questions: z.array(z.string()).length(3)
      })
    });
    
    return followUpResult.object.followup_questions;
  } catch (e) { 
    console.error('Error generating follow-up questions:', e);
    return [];
  }
}

/**
 * ë„êµ¬ ì‹¤í–‰ ê³„íš ìƒì„± í•¨ìˆ˜
 */
// export async function generateToolExecutionPlan(
//   userQuery: string,
//   selectedTools: string[],
//   contextMessages: any[],
//   toolDescriptions: Record<string, string>,
// ): Promise<{
//   plan: string;
//   essentialContext: string;
// }> {    
//     // ì²¨ë¶€íŒŒì¼ ê°ì§€
//     const hasAttachments = contextMessages.some(msg => {
//       if (Array.isArray(msg.content)) {
//         return msg.content.some((part: any) => part.type === 'image' || part.type === 'file');
//       }
//       // AI SDK 5: parts ë°°ì—´ êµ¬ì¡° ì²´í¬
//       if (Array.isArray(msg.parts)) {
//         return msg.parts.some((part: any) => part.type === 'image' || part.type === 'file');
//       }
//       // í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ experimental_attachments ì²´í¬
//       return Array.isArray((msg as any).experimental_attachments) && (msg as any).experimental_attachments.length > 0;
//     });
    
//     // ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì²¨ë¶€íŒŒì¼ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
//     const currentDate = new Date().toLocaleDateString('en-US', { 
//       year: 'numeric', 
//       month: 'long', 
//       day: 'numeric',
//       weekday: 'long'
//     });
    
//     const systemPrompt = `You are an AI assistant analyzing a user's request to create a detailed execution plan for using tools.

// **Current Date:** ${currentDate}
// **User's Request:** ${userQuery}
// **Selected Tools:** ${selectedTools.join(', ')}
// **Available Tools:** ${Object.entries(toolDescriptions).map(([key, desc]) => `${key}: ${desc}`).join('\n')}
// ${hasAttachments ? `**CRITICAL: ATTACHMENTS DETECTED**
// The conversation contains attached files (PDFs, images, documents). You MUST:
// 1. **Extract ALL specific details** from attached files that are relevant to the user's request
// 2. **List every specific entity** (companies, people, places, dates, numbers, etc.) mentioned in attachments
// 3. **Include ALL relevant context** from attachments in your plan
// 4. **Be extremely detailed** about what information to search for based on attachment content
// 5. **Mention specific search terms** and entities that should be researched
// 6. **Consider temporal relevance** - if attachments contain dates, check if information is current as of ${currentDate}

// **Example for company-related requests:**
// - If user asks "check if mentioned companies are listed" and PDF contains companies A, B, C
// - Your plan MUST list: "Search for: Company A, Company B, Company C"
// - Include any additional details from PDF about these companies

// **Example for data analysis requests:**
// - If user asks "analyze this data" and PDF contains specific data points
// - Your plan MUST include: "Search for: [specific data points], [relevant metrics], [related trends]"

// **ATTACHMENT ANALYSIS REQUIREMENTS:**
// - Extract ALL company names, product names, dates, locations, numbers, statistics
// - Include ALL relevant keywords and search terms
// - Be comprehensive - don't miss any important details
// - If attachment contains lists, include ALL items in the list
// - If attachment contains tables, extract ALL relevant data points
// - **Temporal Analysis**: If attachments contain dates, determine if information needs current verification as of ${currentDate}` : ''}

// **Your Task:**
// 1. **Analyze User Intent**: Understand what the user really wants
// 2. **Create Execution Plan**: Detail how to use each selected tool effectively${hasAttachments ? ', including ALL specific details from attachments' : ''}
// 3. **Extract Essential Context**: Identify the most relevant context from conversation history${hasAttachments ? ', with comprehensive details from attached files' : ''}

// **Requirements:**
// - Respond in the user's language
// - Be specific about tool usage order and purpose
// - Focus on what will help the user most
// - Extract only context that directly relates to the current request
// - **Consider temporal relevance** - if the request involves time-sensitive information, prioritize current data as of ${currentDate}${hasAttachments ? `
// - **MANDATORY**: Include ALL specific entities, numbers, dates, and details from attachments
// - **MANDATORY**: List every search term and entity that should be researched
// - **MANDATORY**: Be exhaustive - don't skip any relevant details from attachments
// - **MANDATORY**: For any dates in attachments, determine if current verification is needed as of ${currentDate}` : ''}

// **Output Format:**
// - Plan: Step-by-step tool execution strategy${hasAttachments ? ' with ALL specific details from attachments' : ''}
// - Essential Context: Only the most relevant parts of conversation history${hasAttachments ? ', including comprehensive attachment details' : ''}`;

//     // ğŸ”§ ë©”ì‹œì§€ ë³€í™˜ - unified converter ì‚¬ìš©
//     const convertedMessages = convertToModelMessages(contextMessages);
    
//     const planResult = await generateObject({
//       model: providers.languageModel('gemini-2.0-flash'),
//       system: systemPrompt,
//       messages: convertedMessages,
//       schema: z.object({
//         plan: z.string().describe('Detailed step-by-step plan for tool execution'),
//         essentialContext: z.string().describe('Only the most relevant context from conversation history')
//       })
//     });
    
//     return planResult.object;
// }

/**
 * ê³µí†µ ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜ - ì—ì´ì „íŠ¸ ëª¨ë“œì™€ ì¼ë°˜ ëª¨ë“œì—ì„œ ê³µí†µìœ¼ë¡œ ì‚¬ìš©
 */
export async function processMessagesForAI(messagesWithTokens: any[], model?: string): Promise<ModelMessage[]> {
  
  // GPT-5 ëª¨ë¸ì¸ì§€ í™•ì¸
  const isGPT5 = model && model.startsWith('gpt-5') && model !== 'gpt-5-chat-latest';
  
  // ì½”ë“œíŒŒì¼/í…ìŠ¤íŠ¸íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (UIëŠ” íŒŒì¼ë¡œ ìœ ì§€)
  const processedMessages = await Promise.all(messagesWithTokens.map(async (msg: any) => {
    if (!msg.parts || !Array.isArray(msg.parts)) {
      return msg;
    }
    
    const processedParts = await Promise.all(msg.parts.map(async (part: any) => {
      // GPT-5ì˜ ê²½ìš° reasoning ë°ì´í„°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
      if (part.type === 'reasoning') {
        if (isGPT5) {
          return part; // GPT-5ì—ì„œëŠ” reasoning ë°ì´í„° ìœ ì§€
        } else {
          return {
            type: 'text',
            text: part.reasoningText || part.text || ''
          };
        }
      }
      
      // AI SDK v4 í˜•ì‹ ì´ë¯¸ì§€ë¥¼ v5 í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      if (part.type === 'image' && part.image) {
        // experimental_attachmentsì—ì„œ ì •í™•í•œ mediaTypeê³¼ filename ì°¾ê¸°
        const attachment = msg.experimental_attachments?.find((att: any) => 
          att.url === part.image || att.url.includes(part.image) || part.image.includes(att.url)
        );
        
        return {
          type: 'file',
          url: part.image,
          mediaType: attachment?.contentType || 'image/png',
          filename: attachment?.name || 'image'
        };
      }
      
      if (part.type === 'file' && part.url) {
        // PDFëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        if (part.mediaType === 'application/pdf') {
          return part;
        }
        
        // ì´ë¯¸ì§€ë„ ê·¸ëŒ€ë¡œ ìœ ì§€
        if (part.mediaType && part.mediaType.startsWith('image/')) {
          return part;
        }
        
        // ì½”ë“œíŒŒì¼/í…ìŠ¤íŠ¸íŒŒì¼ (mediaTypeì´ ì—†ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° í¬í•¨)
        // ë‚´ìš©ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        try {
          const fileContent = await fetchFileContent(part.url);
          if (fileContent && fileContent.text) {
            return {
              type: 'text',
              text: `íŒŒì¼ëª…: ${part.filename || 'unknown'}\në‚´ìš©:\n\`\`\`\n${fileContent.text}\n\`\`\``
            };
          } else if (fileContent && fileContent.base64) {
            // PDF íŒŒì¼ì¸ ê²½ìš° (fallback)
            return {
              type: 'file',
              url: part.url,
              mediaType: 'application/pdf',
              filename: part.filename || 'document.pdf'
            };
          }
        } catch (error) {
          console.error('íŒŒì¼ ë‚´ìš© ì½ê¸° ì‹¤íŒ¨:', error);
          return {
            type: 'text',
            text: `íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: ${part.filename || 'unknown'}`
          };
        }
      }
      return part;
    }));
    
    return {
      ...msg,
      parts: processedParts
    };
  }));
  
  const result = convertToModelMessages(processedMessages);
  return result;
}

/**
 * Kimi K2 ëª¨ë¸ í˜¸í™˜ì„±: completion ê°ì²´ì—ì„œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
 */
export function extractTextFromCompletion(completion: any): string {
  try {
    // ìš°ì„ ìˆœìœ„: text > parts > steps > ë¹ˆ ë¬¸ìì—´
    if (completion.text && typeof completion.text === 'string') {
      return completion.text;
    }
    
    if (completion.parts && Array.isArray(completion.parts)) {
      const textParts = completion.parts
        .filter((part: any) => part.type === 'text' && part.text)
        .map((part: any) => part.text);
      if (textParts.length > 0) {
        return textParts.join('\n');
      }
    }
    
    if (completion.steps && Array.isArray(completion.steps)) {
      const textSteps = completion.steps
        .map((step: any) => step.text)
        .filter((text: any) => text && typeof text === 'string');
      if (textSteps.length > 0) {
        return textSteps.join('\n\n');
      }
    }
    
    // ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
    console.warn('âš ï¸ [COMPLETION] Could not extract text from completion object:', Object.keys(completion));
    return '';
  } catch (error) {
    console.error('ğŸ’¥ [COMPLETION] Error extracting text from completion:', error);
    return '';
  }
}
